\section{Related Work}\label{sec:related-work}
There are two main categories of related work. This section first discusses work on FHE vectorization, and then discusses more general approaches to plaintext vectorization 
%\raghav{Your favorite part: finding a better way to taxonomize all of these!}

\subsection{Compilation and Vectorization for FHE}
Prior work has been done on building vectorizing compilers for FHE applications \cite{CHET, Porcupine}.
CHET \cite{CHET} is a vectorizing compiler for homomorphic tensor programs that automatically selects encryption parameters, and chooses optimal data layout strategies.
CHET is specifically targeted towards optimizing the dense tensor computations found in neural network inference, and does not apply to a broader class of programs, especially those with irregular computations that are not so easily vectorized.
\system makes no assumptions about the domain of the program, and can generalize to vectorizing even highly irregular computations.

Porcupine~\cite{Porcupine} is a vectorizing compiler that uses a sketch-based synthesis approach to generate vectorized kernels given a reference implementation. While Porcupine is more general than CHET, it relies on programmers providing partial implementations in the form of sketches, making it less automatic than Coyote. Furthermore, Porcupine relies on the sketches to constrain the search space of rotations, while Coyote is specifically designed to reduce the number of rotations.

%Porcupine \cite{Porcupine} is another vectorizing compiler that uses a synthesis-based approach to automatically generate vectorized FHE kernels given a reference implementation.
%Porcupine is more general than CHET, but it is still mostly targeted towards regular, easily vectorizable computations.
%While Porcupine can, in theory, generate kernels for any computation, its treatment of rotations makes it harder to adapt to vectorizing irregular programs.
%Porcupine considers rotations directly as inputs to arithmetic expressions, and relies on a sketch to constrain the search space and a solver to find programs with optimal performance characteristics.
%Irregular applications without many apparent symmetries to exploit incur many asymmetric rotations, making it more difficult for the solver to produce an optimal schedule. The search space becomes prohibitively large, and is not tuned towards finding minimal sets of rotations.
%In contrast, because \system focuses on limiting rotations, its schedule search prioritizes vector schedules that {\em specifically} reduce the number of required rotations.
%% \raghav{Maybe another thing to say (emphasize?) is that the \system and Porcupine approaches are fundamentally different in that the latter relies on a sketch to synthesize good schedules while \system finds them automatically.}
%
%Additionally, Porcupine's synthesis-based approach results in very long compile times for programs with more than a few instructions.
%Their solution to this problem is to break large programs into multiple smaller, composable kernels and synthesize solutions to each kernel independently.
%This has two major drawbacks: First, the burden of decomposing the program falls on the programmer. Second, synthesizing kernels separately means that Porcupine is unable to find optimizations across kernel boundaries, so a poor decomposition can preclude a more efficient vectorization.
%\system, on the other hand, can compile much larger programs in a reasonable amount of time, and the circuit quotienting strategy automatically breaks large programs up into smaller sequences of vectorizable instructions. %\raghav{TODO: add instruction counts to the eval?}

%\raghav{I think I explained that correctly?}\milind{Sounds good.}

Gazelle \cite{Gazelle} is a framework for secure neural network inference in FHE.
While it is very optimized for a particular use case, Gazelle is not {\em general}: it consists of a library of highly efficient vectorized kernels that are useful in neural network applications.
By contrast, \system can take arbitrary kernels and generate efficient vectorized code on the fly. %\raghav{Not sure how to put this nicely, actually.}

Lee, et. al \cite{CircuitRewriting} describe a general method for automatically rewriting arithmetic and boolean FHE circuits according to a cost model by learning semantically sound rewrite rules.
This approach explores the space of scalar rewrites but does not directly deal with vectorization, making it orthogonal to ours: a technique like this could first be applied to an arbitrary computation to transform it into one more amenable to vectorization before applying \system.

One class of related work consists of compilers \cite{Ramparts, ALCHEMY, EVA, Cingulata} that automatically lift programs written in a high level DSL into optimized FHE circuits that perform the same computation.
Unlike \system, these circuit optimizations do not include automatic vectorization. Although ACLHEMY~\cite{ALCHEMY} and EVA~\cite{EVA} \textit{do} support ciphertext packing, they require the programmer to perform the vectorization.
%\raghav{Remove this since I folded EVA into the (ramparts, alchemy, etc.) sentence earlier?}
%Encrypted Vector Arithmetic (EVA) \cite{EVA} is another Python DSL that aims to make FHE programming more accessible; in particular, it also supports writing vector computation.
%However, unlike \system, EVA requires the programmer to write vectorized code directly: it does not support automatically vectorizing arbitrary kernels.
%\raghav{Mention that EVA automatically does parameter selection? Or the fact that we could potentially target EVA as a backend if it weren't restricted to CKKS?}\milind{nah.}

\subsection{General Purpose Vectorization in non-FHE Settings}
Superword-Level Parallelism is a technique for automatically vectorizing programs~\cite{SLP}.
SLP iterates over a sequence of scalar instructions and computes ``vector packs,'' or sets of isomorphic instructions that can be packed together into vectors.
Because SLP does not rely on the presence of data-parallel structures like loops to aid in vectorization, it works well even for irregular programs.
When computing vector packs, SLP does not account for how expensive rotations are in FHE, leading to schedules with very high data shuffling costs. 

VeGen \cite{VeGen} is a recent variant of SLP introduces {\em lane level parallelism} that reasons about which lanes are performing which computations, allowing it to reason about rotation costs when building vector packs.
For example, VeGen can reason about the rotation costs to pack together operands for an instruction into a temporary vector, and can use this to decide whether or not packing those instructions is worth it.
However, this reasoning only happens locally, and VeGen does not incorporate information about how instruction packing might affect rotations much later in the program.

%\raghav{Spotcheck again?}
goSLP \cite{goSLP} reasons about globally optimal packing, and tries to find lane placements that minimize data shuffling costs. 
However, there are assumptions baked into its cost model that make it fundamentally unsuitable for the FHE setting.
goSLP frames vectorization overhead in terms of the number of {\em pack} and {\em unpack} operations incurred.
For example, permuting the slots of a single vector incurs one unpack, whereas blending the contents of N vectors (without any permutation) incurs N unpacks.
This results in a cost model that implicitly requires wide blends to be much more expensive than arbitrary permutations, almost the opposite of FHE's cost model. In FHE, blends are almost free (instantiated as cheap plaintext multiplies and ciphertext adds) whereas a ``bad'' permutation can require O(n) rotates to realize.
In other words, in the FHE world, there are often highly profitable schedules that require many blends and few rotates, but the framing goSLP uses for vectorization overhead will cause it to forego these schedules for more conservative ones.
Additionally, goSLP does lane placement (permutation selection) {\em after} finding vector packs, creating situations like the one we described in Section~\ref{sec:intro} in which the ostensibly profitable packing does not admit a good data layout.
By contrast, \system's cooperative scheduling strategy ensures that this does not happen.

% While goSLP does reason globally about lane placement, it has two drawbacks in an FHE context. First, it packs instructions into vectors before considering shuffling costs, so may over-aggressively vectorize, while \system may forego some vectorization opportunities in the name of getting packing larger expressions without rotation. Second, its lane placement algorithm considers arbitrary permutations, which are impractically expensive in FHE.
% \raghav{goSLP frames vectorization overhead in terms of the number of {\em pack} and {\em unpack} operations incurred. For example, permuting the slots of a single vector incurs one unpack, whereas blending the contents of N vectors (without any permutation) incurs N unpacks. In other words, goSLP bakes in the assumption that wide blends are much more expensive than arbitrary permutations. This is fundamentally incompatible with FHE, since blends are almost free, whereas bad permutations can require several expensive rotations to realize. Essentially, in the FHE world, its often better to produce a schedule that {\em looks much worse} than to produce the traditionally optimal one.}

Swizzle Inventor \cite{SwizzleInventor} is a system which automatically synthesizes efficient data movement kernels for vectorized GPU code.
There are two primary obstacles \system faces in directly applying this approach to our setting.
First, SwizzleInventor tackles the data movement problem {\em after the kernel has been vectorized}. As we discussed earlier, in the world of FHE, packing and data movement are problems that cannot be reasoned about separately, and must be addressed together.
Second, the sketches SwizzleInventor uses to guide its synthesis rely on the ability to efficiently access arbitrary slots of a packed vector. However, this is not possible with FHE vectors without incurring significant rotation overheads.

Diospyros~\cite{Diospyros} is an equality saturation--based vectorization strategy that constructs an e-graph~\cite{EqualitySaturation, egg} of programs that are semantically equivalent to a given specification, and then uses a custom cost model to extract an efficient vector program, together with necessary shuffles.
However, the simplicity of the cost model it associates to various shuffles makes it unsuitable to deal with the peculiarities and inflexibility of FHE rotations.

%By reasoning globally about the entire dependence graph of the program at once, \system can identify such phenomena when making vector packs.
%\system can also automatically find vector schedules that are amenable to more efficient data movement, and can propagate this information back through the program this to identify more optimal data layouts.

% Talk about SLP
% Talk about FHE (Gentry paper)
% Talk in depth about CHET
% Talk in depth about Porcupine